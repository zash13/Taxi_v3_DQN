{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-27T16:01:22.918912Z","iopub.execute_input":"2025-07-27T16:01:22.919583Z","iopub.status.idle":"2025-07-27T16:01:24.843746Z","shell.execute_reply.started":"2025-07-27T16:01:22.919553Z","shell.execute_reply":"2025-07-27T16:01:24.843011Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from os import stat\nimport numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras.optimizers import Adam\nimport random\nfrom tensorflow.keras.layers import Dense, Input\nfrom collections import deque\nfrom enum import Enum\nimport gymnasium as gym\n\n\nclass EpsilonPolicyType(Enum):\n    NONE = 0\n    DECAY = 1\n    SOFTLINEAR = 2\n\n\nclass RewardPolicyType(Enum):\n    NONE = 0\n    ERM = 1\n\n\nINFINITY = float(\"inf\")\n\n\nclass ExperienceBuffer:\n    def __init__(\n        self,\n        rewarder: \"RewardHelper\",\n        buffer_size=2000,\n        prefer_lower_heuristic=True,\n    ) -> None:\n        self.memory_buffer = deque(maxlen=buffer_size)\n        self.buffer_size = buffer_size\n        self.reward_helper = rewarder\n        self.prefer_lower_heuristic = prefer_lower_heuristic\n\n    def store_experience(\n        self, current_state, next_state, imm_reward, action, done, heuristic=None\n    ):\n        imm_reward = self.reward_helper.findReward(\n            current_state, imm_reward, heuristic, self.prefer_lower_heuristic\n        )\n        self.memory_buffer.append(\n            {\n                \"current_state\": current_state,\n                \"action\": action,\n                \"reward\": imm_reward,\n                \"next_state\": next_state,\n                \"heuristic\": heuristic,\n                \"done\": done,\n            }\n        )\n\n    def sample_batch(self, count):\n        if len(self.memory_buffer) < count:\n            return None\n        batch = random.sample(self.memory_buffer, count)\n        states = np.vstack([item[\"current_state\"] for item in batch])\n        next_states = np.vstack([item[\"next_state\"] for item in batch])\n        rewards = np.array([item[\"reward\"] for item in batch])\n        actions = np.array([item[\"action\"] for item in batch])\n        dones = np.array([item[\"done\"] for item in batch])\n        heuristics = np.array([item[\"heuristic\"] for item in batch])\n        return states, next_states, rewards, actions, dones, heuristics\n\n\nclass QNetwork:\n    def __init__(self, state_size, action_size, learning_rate=0.001) -> None:\n        self.state_size = state_size\n        self.action_size = action_size\n        self._model = self._initiate_model()\n        self._model.compile(loss=\"mse\", optimizer=Adam(learning_rate=learning_rate))\n\n    def _initiate_model(self):\n        return keras.Sequential(\n            [\n                Input(shape=(self.state_size,)),\n                Dense(units=64, activation=\"relu\"),\n                Dense(units=32, activation=\"relu\"),\n                Dense(units=32, activation=\"relu\"),\n                Dense(units=self.action_size, activation=\"linear\"),\n            ]\n        )\n\n    def predict(self, states, verbose=0):\n        return self._model.predict(states, verbose=verbose)\n\n    def fit(self, states, q_targets, epochs=1, verbose=0):\n        history = self._model.fit(states, q_targets, epochs=epochs, verbose=verbose)\n        return history.history[\"loss\"][0]\n\n\n# we learnd that agent should balance between exploration and exploitation\n# without this section , it seems that agent just try to explor new path\n#\nclass EpsilonPolicy:\n    def __init__(\n        self,\n        epsilon_min: float = 0.01,\n        epsilon_decay: float = 0.995,\n        policy: EpsilonPolicyType = EpsilonPolicyType.DECAY,\n        update_per_episod: bool = True,\n    ):\n        self.policy = policy\n        self.visited_states = {}\n        self.epsilon: float = INFINITY\n        self.epsilon_min = epsilon_min\n        self.epsilon_decay = epsilon_decay\n        self.update_per_episod = update_per_episod\n        self.old_episod = 0\n\n    def adjust_epsilon(\n        self,\n        epsilon,\n        episode_count,\n        max_episodes=100,\n    ):\n        self.epsilon = epsilon\n        if self.update_per_episod:\n            if self.old_episod == episode_count:\n                return self.epsilon\n            else:\n                self.old_episod = episode_count\n        if self.policy == EpsilonPolicyType.DECAY:\n            return self.linear_decay(episode_count, max_episodes)\n        elif self.policy == EpsilonPolicyType.SOFTLINEAR:\n            return self.soft_linear_decayr(episode_count, max_episodes)\n        else:\n            return self.epsilon\n\n    def linear_decay(self, episode_count, max_episodes):\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n        self.epsilon = max(self.epsilon, self.epsilon_min)\n        return self.epsilon\n\n    def soft_linear_decayr(self, episode_count, max_episodes=200):\n        if self.epsilon > self.epsilon_min:\n            target_epsilon = max(\n                self.epsilon_min,\n                1.0 - (1.0 - self.epsilon_min) * (episode_count / max_episodes),\n            )\n            self.epsilon = self.epsilon * self.epsilon_decay + target_epsilon * (\n                1 - self.epsilon_decay\n            )\n        return self.epsilon\n\n\nclass RewardHelper:\n    def __init__(\n        self,\n        progress_bonus: float = 0.05,\n        exploration_bonus: float = 0.1,\n        policy: RewardPolicyType = RewardPolicyType.ERM,\n    ):\n        self.progress_bonus = progress_bonus\n        self.exploration_bonus = exploration_bonus\n        self.policy = policy\n        self.old_huristic = INFINITY\n        self.visited_states = {}\n\n    def findReward(self, state, reward, heuristic=None, lowerHuristicBetter=True):\n        if self.policy == RewardPolicyType.ERM:\n            return self._ERM(state, reward, heuristic, lowerHuristicBetter)\n        elif self.policy == RewardPolicyType.NONE:\n            return self._none(reward)\n        else:\n            return reward\n\n    def _none(self, reward):\n        return reward\n\n    def _ERM(self, state, reward, heuristic=None, lowerHuristicBetter=True):\n        state_key = tuple(state.flatten()) if state is not None else None\n        is_new_state = state_key is not None and state_key not in self.visited_states\n        if is_new_state:\n            if is_new_state and state_key is not None:\n                self.visited_states[state_key] = (\n                    heuristic if heuristic is not None else INFINITY\n                )\n\n        progress = 0.0\n        if heuristic is not None and state_key is not None:\n            old_heuristic = self.visited_states[state_key]\n            if (\n                old_heuristic != INFINITY\n                and (heuristic < old_heuristic and lowerHuristicBetter)\n                or (heuristic > old_heuristic and not lowerHuristicBetter)\n            ):\n                progress = self.progress_bonus\n                self.visited_states[state_key] = heuristic\n\n        new_reward = reward\n        if is_new_state:\n            new_reward += self.exploration_bonus\n        if progress > 0:\n            new_reward += self.progress_bonus\n        return new_reward\n\n\nclass DQNAgent:\n    def __init__(\n        self,\n        action_size,\n        state_size,\n        learning_rate=0.001,\n        buffer_size=2000,\n        batch_size=32,\n        gamma=0.99,\n        max_episodes=200,\n        epsilon=1.0,\n        epsilon_min=0.01,\n        epsilon_decay=0.995,\n        epsilon_policy: EpsilonPolicy = None,\n        reward_policy: RewardPolicyType = RewardPolicyType.NONE,\n        prefer_lower_heuristic=True,\n        progress_bonus: float = 0.05,\n        exploration_bonus: float = 0.1,\n    ) -> None:\n        self.action_size = action_size\n        self.state_size = state_size\n        self.gamma = gamma\n        self.batch_size = batch_size\n        self.epsilon = epsilon\n        self.episode_count = 0\n        self.max_episodes = max_episodes\n\n        self.model = QNetwork(state_size, action_size, learning_rate)\n        self.epsilon_policy = epsilon_policy or EpsilonPolicy(\n            epsilon_min=epsilon_min,\n            epsilon_decay=epsilon_decay,\n            policy=EpsilonPolicyType.DECAY,\n        )\n        self.buffer_helper = ExperienceBuffer(\n            RewardHelper(progress_bonus, exploration_bonus, reward_policy),\n            buffer_size=buffer_size,\n            prefer_lower_heuristic=prefer_lower_heuristic,\n        )\n\n    def train(self, episode):\n        data = self.buffer_helper.sample_batch(self.batch_size)\n        if data is None:\n            return None\n        states, next_states, rewards, actions, dones, heuristics = data\n        q_current = self.model.predict(states, verbose=0)\n        q_next = self.model.predict(next_states, verbose=0)\n        q_targets = q_current.copy()\n        for i in range(self.batch_size):\n            if not dones[i]:\n                q_targets[i, actions[i]] = rewards[i] + self.gamma * np.max(q_next[i])\n            else:\n                q_targets[i, actions[i]] = rewards[i]\n\n        self._update_exploration_rate(episode_count=episode)\n        loss = self.model.fit(states, q_targets, epochs=1, verbose=0)\n        return loss\n\n    def _update_exploration_rate(self, episode_count):\n        self.epsilon = self.epsilon_policy.adjust_epsilon(\n            self.epsilon, episode_count=episode_count, max_episodes=self.max_episodes\n        )\n\n    def select_action(self, current_state):\n        if np.random.uniform(0, 1) < self.epsilon:\n            return np.random.choice(self.action_size)\n        else:\n            q_value = self.model.predict(current_state, verbose=0)[0]\n            return np.argmax(q_value)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T16:24:41.693413Z","iopub.execute_input":"2025-07-27T16:24:41.693879Z","iopub.status.idle":"2025-07-27T16:24:41.728777Z","shell.execute_reply.started":"2025-07-27T16:24:41.693849Z","shell.execute_reply":"2025-07-27T16:24:41.727811Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"\nenv = gym.make(\"Taxi-v3\")\n\nstate_size = env.observation_space.n\naction_size = env.action_space.n\n\nepsilon_min = 0.1\nepsilon_decay = 0.995\nep_policy = EpsilonPolicy(\n    epsilon_min,\n    epsilon_decay,\n    policy=EpsilonPolicyType.DECAY,\n)\nnum_episodes = 1000\nrewards = []\nmax_steps = 200\nagent = DQNAgent(\n    action_size=action_size,\n    state_size=state_size,\n    learning_rate=0.001,\n    buffer_size=2000,\n    batch_size=96,\n    gamma=0.99,\n    max_episodes=num_episodes,\n    epsilon=1.0,\n    epsilon_min=0.01,\n    epsilon_decay=0.995,\n    epsilon_policy=ep_policy,\n    reward_policy=RewardPolicyType.ERM,\n    progress_bonus=0.05,\n    exploration_bonus=0.1,\n)\nfor episode in range(num_episodes):\n    state, _ = env.reset()\n    state = np.eye(state_size)[state]\n    total_reward = 0\n    done = False\n    step = 0\n\n    while not done and step < max_steps:\n        action = agent.select_action(np.array([state]))\n\n        next_state, reward, terminated, truncated, _ = env.step(action)\n        next_state = np.eye(state_size)[next_state]\n        done = terminated or truncated\n\n        agent.buffer_helper.store_experience(\n            current_state=state,\n            next_state=next_state,\n            imm_reward=reward,\n            action=action,\n            done=done,\n            heuristic=None,\n        )\n\n        loss = agent.train(episode)\n\n        state = next_state\n        total_reward += reward\n        step += 1\n\n    rewards.append(total_reward)\n    print(\n        f\"Episode {episode}/{num_episodes}, Total Reward: {total_reward}, Epsilon: {agent.epsilon:.3f} , loss: {loss}\"\n    )\n\nenv.close()\n\nimport matplotlib.pyplot as plt\n\nplt.plot(rewards)\nplt.xlabel(\"Episode\")\nplt.ylabel(\"Total Reward\")\nplt.title(\"Training Progress on Taxi-v3\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T16:24:58.194783Z","iopub.execute_input":"2025-07-27T16:24:58.195100Z"}},"outputs":[{"name":"stdout","text":"Episode 0/1000, Total Reward: -866, Epsilon: 1.000 , loss: 0.05654409900307655\nEpisode 1/1000, Total Reward: -830, Epsilon: 0.995 , loss: 0.09423211961984634\nEpisode 2/1000, Total Reward: -695, Epsilon: 0.990 , loss: 0.1615419238805771\nEpisode 3/1000, Total Reward: -839, Epsilon: 0.985 , loss: 0.06783589720726013\nEpisode 4/1000, Total Reward: -704, Epsilon: 0.980 , loss: 0.04250073805451393\nEpisode 5/1000, Total Reward: -875, Epsilon: 0.975 , loss: 0.06581822782754898\nEpisode 6/1000, Total Reward: -821, Epsilon: 0.970 , loss: 0.05833987519145012\nEpisode 7/1000, Total Reward: -668, Epsilon: 0.966 , loss: 0.012292672879993916\nEpisode 8/1000, Total Reward: -785, Epsilon: 0.961 , loss: 0.03861413896083832\nEpisode 9/1000, Total Reward: -812, Epsilon: 0.956 , loss: 0.030897021293640137\nEpisode 10/1000, Total Reward: -848, Epsilon: 0.951 , loss: 0.12225767225027084\nEpisode 11/1000, Total Reward: -695, Epsilon: 0.946 , loss: 0.011467318050563335\nEpisode 12/1000, Total Reward: -704, Epsilon: 0.942 , loss: 0.13225121796131134\nEpisode 13/1000, Total Reward: -722, Epsilon: 0.937 , loss: 0.01871047355234623\nEpisode 14/1000, Total Reward: -560, Epsilon: 0.932 , loss: 0.02551168203353882\nEpisode 15/1000, Total Reward: -902, Epsilon: 0.928 , loss: 0.021655378863215446\nEpisode 16/1000, Total Reward: -749, Epsilon: 0.923 , loss: 0.28601133823394775\nEpisode 17/1000, Total Reward: -668, Epsilon: 0.918 , loss: 0.008062739856541157\nEpisode 18/1000, Total Reward: -740, Epsilon: 0.914 , loss: 0.11128765344619751\nEpisode 19/1000, Total Reward: -731, Epsilon: 0.909 , loss: 0.12218870967626572\nEpisode 20/1000, Total Reward: -785, Epsilon: 0.905 , loss: 0.014379664324223995\nEpisode 21/1000, Total Reward: -659, Epsilon: 0.900 , loss: 0.02001110650599003\nEpisode 22/1000, Total Reward: -839, Epsilon: 0.896 , loss: 0.14500439167022705\nEpisode 23/1000, Total Reward: -695, Epsilon: 0.891 , loss: 0.014428503811359406\nEpisode 24/1000, Total Reward: -722, Epsilon: 0.887 , loss: 0.014789932407438755\nEpisode 25/1000, Total Reward: -704, Epsilon: 0.882 , loss: 0.17082400619983673\nEpisode 26/1000, Total Reward: -713, Epsilon: 0.878 , loss: 0.14773525297641754\nEpisode 27/1000, Total Reward: -731, Epsilon: 0.873 , loss: 0.014968114905059338\nEpisode 28/1000, Total Reward: -668, Epsilon: 0.869 , loss: 0.06920614093542099\nEpisode 29/1000, Total Reward: -731, Epsilon: 0.865 , loss: 0.020082900300621986\nEpisode 30/1000, Total Reward: -713, Epsilon: 0.860 , loss: 0.029770992696285248\nEpisode 31/1000, Total Reward: -668, Epsilon: 0.856 , loss: 0.017156513407826424\nEpisode 32/1000, Total Reward: -686, Epsilon: 0.852 , loss: 0.3489780128002167\nEpisode 33/1000, Total Reward: -695, Epsilon: 0.848 , loss: 0.02553372271358967\nEpisode 34/1000, Total Reward: -731, Epsilon: 0.843 , loss: 0.6676738858222961\nEpisode 35/1000, Total Reward: -785, Epsilon: 0.839 , loss: 0.03648809716105461\nEpisode 36/1000, Total Reward: -695, Epsilon: 0.835 , loss: 0.013369563035666943\nEpisode 37/1000, Total Reward: -659, Epsilon: 0.831 , loss: 0.026978641748428345\nEpisode 38/1000, Total Reward: -695, Epsilon: 0.827 , loss: 0.030947020277380943\nEpisode 39/1000, Total Reward: -686, Epsilon: 0.822 , loss: 0.5578150153160095\nEpisode 40/1000, Total Reward: -695, Epsilon: 0.818 , loss: 0.09853273630142212\nEpisode 41/1000, Total Reward: -785, Epsilon: 0.814 , loss: 0.0372905395925045\nEpisode 42/1000, Total Reward: -704, Epsilon: 0.810 , loss: 0.3524450361728668\nEpisode 43/1000, Total Reward: -587, Epsilon: 0.806 , loss: 0.04060230776667595\nEpisode 44/1000, Total Reward: -731, Epsilon: 0.802 , loss: 0.030918734148144722\nEpisode 45/1000, Total Reward: -803, Epsilon: 0.798 , loss: 0.04853197932243347\nEpisode 46/1000, Total Reward: -785, Epsilon: 0.794 , loss: 0.07696612924337387\nEpisode 47/1000, Total Reward: -713, Epsilon: 0.790 , loss: 0.5739647150039673\nEpisode 48/1000, Total Reward: -170, Epsilon: 0.786 , loss: 0.15951001644134521\nEpisode 49/1000, Total Reward: -686, Epsilon: 0.782 , loss: 0.1374906450510025\nEpisode 50/1000, Total Reward: -650, Epsilon: 0.778 , loss: 0.06464626640081406\nEpisode 51/1000, Total Reward: -704, Epsilon: 0.774 , loss: 0.07827483862638474\nEpisode 52/1000, Total Reward: -659, Epsilon: 0.771 , loss: 0.031083449721336365\nEpisode 53/1000, Total Reward: -713, Epsilon: 0.767 , loss: 0.21706663072109222\nEpisode 54/1000, Total Reward: -587, Epsilon: 0.763 , loss: 0.16934460401535034\nEpisode 55/1000, Total Reward: -731, Epsilon: 0.759 , loss: 0.033950936049222946\nEpisode 56/1000, Total Reward: -623, Epsilon: 0.755 , loss: 0.05572960898280144\nEpisode 57/1000, Total Reward: -524, Epsilon: 0.751 , loss: 0.15485651791095734\nEpisode 58/1000, Total Reward: -740, Epsilon: 0.748 , loss: 0.0416017584502697\nEpisode 59/1000, Total Reward: -749, Epsilon: 0.744 , loss: 0.05394834280014038\nEpisode 60/1000, Total Reward: -307, Epsilon: 0.740 , loss: 0.046112772077322006\nEpisode 61/1000, Total Reward: -179, Epsilon: 0.737 , loss: 1.9636335372924805\nEpisode 62/1000, Total Reward: -668, Epsilon: 0.733 , loss: 0.09183108061552048\nEpisode 63/1000, Total Reward: -569, Epsilon: 0.729 , loss: 0.44355523586273193\nEpisode 64/1000, Total Reward: -605, Epsilon: 0.726 , loss: 0.2855188846588135\nEpisode 65/1000, Total Reward: -596, Epsilon: 0.722 , loss: 0.9791688323020935\nEpisode 66/1000, Total Reward: -623, Epsilon: 0.718 , loss: 186.00523376464844\nEpisode 67/1000, Total Reward: -578, Epsilon: 0.715 , loss: 28978.318359375\nEpisode 68/1000, Total Reward: -632, Epsilon: 0.711 , loss: 333745.65625\nEpisode 69/1000, Total Reward: -533, Epsilon: 0.708 , loss: 2271380.5\nEpisode 70/1000, Total Reward: -713, Epsilon: 0.704 , loss: 2888561.0\nEpisode 71/1000, Total Reward: -551, Epsilon: 0.701 , loss: 3621952.0\nEpisode 72/1000, Total Reward: -668, Epsilon: 0.697 , loss: 3464763.0\nEpisode 73/1000, Total Reward: -596, Epsilon: 0.694 , loss: 27629110.0\nEpisode 74/1000, Total Reward: -767, Epsilon: 0.690 , loss: 6360288.0\nEpisode 75/1000, Total Reward: -1199, Epsilon: 0.687 , loss: 11871937.0\nEpisode 76/1000, Total Reward: -731, Epsilon: 0.683 , loss: 23289138.0\nEpisode 77/1000, Total Reward: -614, Epsilon: 0.680 , loss: 56910044.0\nEpisode 78/1000, Total Reward: -605, Epsilon: 0.676 , loss: 7293630.0\nEpisode 79/1000, Total Reward: -632, Epsilon: 0.673 , loss: 80780832.0\nEpisode 80/1000, Total Reward: -677, Epsilon: 0.670 , loss: 48656132.0\nEpisode 81/1000, Total Reward: -632, Epsilon: 0.666 , loss: 3821015.25\nEpisode 82/1000, Total Reward: -578, Epsilon: 0.663 , loss: 7353872.5\nEpisode 83/1000, Total Reward: -650, Epsilon: 0.660 , loss: 10723523.0\nEpisode 84/1000, Total Reward: -506, Epsilon: 0.656 , loss: 8901557.0\nEpisode 85/1000, Total Reward: -596, Epsilon: 0.653 , loss: 51555972.0\nEpisode 86/1000, Total Reward: -143, Epsilon: 0.650 , loss: 149964912.0\nEpisode 87/1000, Total Reward: -623, Epsilon: 0.647 , loss: 12783021.0\nEpisode 88/1000, Total Reward: -596, Epsilon: 0.643 , loss: 57158144.0\nEpisode 89/1000, Total Reward: -821, Epsilon: 0.640 , loss: 72793112.0\nEpisode 90/1000, Total Reward: -587, Epsilon: 0.637 , loss: 13120405.0\nEpisode 91/1000, Total Reward: -596, Epsilon: 0.634 , loss: 49032260.0\nEpisode 92/1000, Total Reward: -677, Epsilon: 0.631 , loss: 50321660.0\nEpisode 93/1000, Total Reward: -839, Epsilon: 0.627 , loss: 197355904.0\nEpisode 94/1000, Total Reward: -542, Epsilon: 0.624 , loss: 2447136.25\nEpisode 95/1000, Total Reward: -938, Epsilon: 0.621 , loss: 4293706.0\nEpisode 96/1000, Total Reward: -623, Epsilon: 0.618 , loss: 590203.4375\nEpisode 97/1000, Total Reward: -713, Epsilon: 0.615 , loss: 475330.96875\nEpisode 98/1000, Total Reward: -785, Epsilon: 0.612 , loss: 3543889.75\nEpisode 99/1000, Total Reward: -875, Epsilon: 0.609 , loss: 7369510.0\nEpisode 100/1000, Total Reward: -515, Epsilon: 0.606 , loss: 5331281.5\nEpisode 101/1000, Total Reward: -408, Epsilon: 0.603 , loss: 367856.09375\nEpisode 102/1000, Total Reward: -632, Epsilon: 0.600 , loss: 894283.0\nEpisode 103/1000, Total Reward: -533, Epsilon: 0.597 , loss: 4144377.25\nEpisode 104/1000, Total Reward: -596, Epsilon: 0.594 , loss: 39720264.0\nEpisode 105/1000, Total Reward: -605, Epsilon: 0.591 , loss: 27234168.0\nEpisode 106/1000, Total Reward: -1064, Epsilon: 0.588 , loss: 6031652.0\nEpisode 107/1000, Total Reward: -614, Epsilon: 0.585 , loss: 3690757.25\nEpisode 108/1000, Total Reward: -668, Epsilon: 0.582 , loss: 11370971.0\nEpisode 109/1000, Total Reward: -533, Epsilon: 0.579 , loss: 4193146.75\nEpisode 110/1000, Total Reward: -569, Epsilon: 0.576 , loss: 7194643.5\nEpisode 111/1000, Total Reward: -695, Epsilon: 0.573 , loss: 23590262.0\nEpisode 112/1000, Total Reward: -533, Epsilon: 0.570 , loss: 2370098.0\nEpisode 113/1000, Total Reward: -659, Epsilon: 0.568 , loss: 1329627.375\nEpisode 114/1000, Total Reward: -587, Epsilon: 0.565 , loss: 19787400.0\nEpisode 115/1000, Total Reward: -713, Epsilon: 0.562 , loss: 22650074.0\nEpisode 116/1000, Total Reward: -722, Epsilon: 0.559 , loss: 27652166.0\nEpisode 117/1000, Total Reward: -749, Epsilon: 0.556 , loss: 1084461.0\nEpisode 118/1000, Total Reward: -830, Epsilon: 0.554 , loss: 5357598.5\nEpisode 119/1000, Total Reward: -342, Epsilon: 0.551 , loss: 24908130.0\nEpisode 120/1000, Total Reward: -731, Epsilon: 0.548 , loss: 2641330.75\nEpisode 121/1000, Total Reward: -857, Epsilon: 0.545 , loss: 281686.15625\nEpisode 122/1000, Total Reward: -659, Epsilon: 0.543 , loss: 439355.34375\nEpisode 123/1000, Total Reward: -623, Epsilon: 0.540 , loss: 321563.875\nEpisode 124/1000, Total Reward: -776, Epsilon: 0.537 , loss: 1988044.125\nEpisode 125/1000, Total Reward: -623, Epsilon: 0.534 , loss: 1096813.875\nEpisode 126/1000, Total Reward: -677, Epsilon: 0.532 , loss: 1028160.25\nEpisode 127/1000, Total Reward: -668, Epsilon: 0.529 , loss: 392694.125\nEpisode 128/1000, Total Reward: -614, Epsilon: 0.526 , loss: 663134.8125\nEpisode 129/1000, Total Reward: -668, Epsilon: 0.524 , loss: 332331.34375\nEpisode 130/1000, Total Reward: -569, Epsilon: 0.521 , loss: 24070344.0\nEpisode 131/1000, Total Reward: -686, Epsilon: 0.519 , loss: 6868265.5\nEpisode 132/1000, Total Reward: -542, Epsilon: 0.516 , loss: 2961290.0\nEpisode 133/1000, Total Reward: -443, Epsilon: 0.513 , loss: 1741958.375\nEpisode 134/1000, Total Reward: -443, Epsilon: 0.511 , loss: 2160083.75\nEpisode 135/1000, Total Reward: -668, Epsilon: 0.508 , loss: 14137315.0\nEpisode 136/1000, Total Reward: -470, Epsilon: 0.506 , loss: 18245342.0\nEpisode 137/1000, Total Reward: -542, Epsilon: 0.503 , loss: 35583420.0\nEpisode 138/1000, Total Reward: -461, Epsilon: 0.501 , loss: 11618233.0\nEpisode 139/1000, Total Reward: -533, Epsilon: 0.498 , loss: 21163696.0\nEpisode 140/1000, Total Reward: -443, Epsilon: 0.496 , loss: 1537318.875\nEpisode 141/1000, Total Reward: -740, Epsilon: 0.493 , loss: 1317788.625\nEpisode 142/1000, Total Reward: -659, Epsilon: 0.491 , loss: 4581815.0\nEpisode 143/1000, Total Reward: -704, Epsilon: 0.488 , loss: 2801284.0\nEpisode 144/1000, Total Reward: -587, Epsilon: 0.486 , loss: 1989497.375\nEpisode 145/1000, Total Reward: -443, Epsilon: 0.483 , loss: 1008185.8125\nEpisode 146/1000, Total Reward: -497, Epsilon: 0.481 , loss: 229682.875\nEpisode 147/1000, Total Reward: -506, Epsilon: 0.479 , loss: 99868.9140625\nEpisode 148/1000, Total Reward: -497, Epsilon: 0.476 , loss: 979331.0625\nEpisode 149/1000, Total Reward: -506, Epsilon: 0.474 , loss: 238350.0625\nEpisode 150/1000, Total Reward: -695, Epsilon: 0.471 , loss: 305982.78125\nEpisode 151/1000, Total Reward: -569, Epsilon: 0.469 , loss: 3451352.75\nEpisode 152/1000, Total Reward: -497, Epsilon: 0.467 , loss: 164926.875\nEpisode 153/1000, Total Reward: -974, Epsilon: 0.464 , loss: 564832.0\nEpisode 154/1000, Total Reward: -524, Epsilon: 0.462 , loss: 21074.2734375\nEpisode 155/1000, Total Reward: -533, Epsilon: 0.460 , loss: 27143.474609375\nEpisode 156/1000, Total Reward: -452, Epsilon: 0.458 , loss: 63491.484375\nEpisode 157/1000, Total Reward: -515, Epsilon: 0.455 , loss: 206957.796875\nEpisode 158/1000, Total Reward: -578, Epsilon: 0.453 , loss: 199562.890625\nEpisode 159/1000, Total Reward: -758, Epsilon: 0.451 , loss: 28535.634765625\nEpisode 160/1000, Total Reward: -587, Epsilon: 0.448 , loss: 56700.234375\nEpisode 161/1000, Total Reward: -893, Epsilon: 0.446 , loss: 36437.078125\nEpisode 162/1000, Total Reward: -650, Epsilon: 0.444 , loss: 557212.1875\nEpisode 163/1000, Total Reward: -812, Epsilon: 0.442 , loss: 1536757.375\nEpisode 164/1000, Total Reward: -578, Epsilon: 0.440 , loss: 158202.296875\nEpisode 165/1000, Total Reward: -623, Epsilon: 0.437 , loss: 179352.796875\nEpisode 166/1000, Total Reward: -587, Epsilon: 0.435 , loss: 326465.59375\nEpisode 167/1000, Total Reward: -560, Epsilon: 0.433 , loss: 62164.65625\nEpisode 168/1000, Total Reward: -839, Epsilon: 0.431 , loss: 279422.15625\nEpisode 169/1000, Total Reward: -169, Epsilon: 0.429 , loss: 4417372.5\nEpisode 170/1000, Total Reward: -794, Epsilon: 0.427 , loss: 903658.3125\nEpisode 171/1000, Total Reward: -1001, Epsilon: 0.424 , loss: 216473.4375\nEpisode 172/1000, Total Reward: -875, Epsilon: 0.422 , loss: 389300.40625\nEpisode 173/1000, Total Reward: -614, Epsilon: 0.420 , loss: 163279.515625\nEpisode 174/1000, Total Reward: -650, Epsilon: 0.418 , loss: 2927842.75\nEpisode 175/1000, Total Reward: -713, Epsilon: 0.416 , loss: 1556712.625\nEpisode 176/1000, Total Reward: -659, Epsilon: 0.414 , loss: 154490.390625\nEpisode 177/1000, Total Reward: -776, Epsilon: 0.412 , loss: 281002.875\nEpisode 178/1000, Total Reward: -159, Epsilon: 0.410 , loss: 75331.59375\nEpisode 179/1000, Total Reward: -434, Epsilon: 0.408 , loss: 315294.46875\nEpisode 180/1000, Total Reward: -497, Epsilon: 0.406 , loss: 11886.3232421875\nEpisode 181/1000, Total Reward: -371, Epsilon: 0.404 , loss: 23948.728515625\nEpisode 182/1000, Total Reward: -425, Epsilon: 0.402 , loss: 17613.048828125\nEpisode 183/1000, Total Reward: -335, Epsilon: 0.400 , loss: 9109.0361328125\nEpisode 184/1000, Total Reward: -515, Epsilon: 0.398 , loss: 28104.4375\nEpisode 185/1000, Total Reward: -605, Epsilon: 0.396 , loss: 32825.51171875\nEpisode 186/1000, Total Reward: -452, Epsilon: 0.394 , loss: 70865.75\nEpisode 187/1000, Total Reward: -560, Epsilon: 0.392 , loss: 46042.16796875\nEpisode 188/1000, Total Reward: -253, Epsilon: 0.390 , loss: 312441.71875\nEpisode 189/1000, Total Reward: -497, Epsilon: 0.388 , loss: 52218.35546875\nEpisode 190/1000, Total Reward: -479, Epsilon: 0.386 , loss: 11577.3427734375\nEpisode 191/1000, Total Reward: -704, Epsilon: 0.384 , loss: 47694.90234375\nEpisode 192/1000, Total Reward: -641, Epsilon: 0.382 , loss: 18003.048828125\nEpisode 193/1000, Total Reward: -821, Epsilon: 0.380 , loss: 4773.55419921875\nEpisode 194/1000, Total Reward: -596, Epsilon: 0.378 , loss: 17472.927734375\nEpisode 195/1000, Total Reward: -641, Epsilon: 0.376 , loss: 8575.5322265625\nEpisode 196/1000, Total Reward: -569, Epsilon: 0.374 , loss: 13934.2939453125\nEpisode 197/1000, Total Reward: -587, Epsilon: 0.373 , loss: 2039.9761962890625\nEpisode 198/1000, Total Reward: -506, Epsilon: 0.371 , loss: 7975.359375\nEpisode 199/1000, Total Reward: -443, Epsilon: 0.369 , loss: 4893.7412109375\nEpisode 200/1000, Total Reward: -551, Epsilon: 0.367 , loss: 5453.46533203125\nEpisode 201/1000, Total Reward: -722, Epsilon: 0.365 , loss: 2726.12744140625\nEpisode 202/1000, Total Reward: -578, Epsilon: 0.363 , loss: 3883.255859375\nEpisode 203/1000, Total Reward: -695, Epsilon: 0.361 , loss: 5369.47998046875\nEpisode 204/1000, Total Reward: -515, Epsilon: 0.360 , loss: 3119.2998046875\nEpisode 205/1000, Total Reward: -461, Epsilon: 0.358 , loss: 9013.7275390625\nEpisode 206/1000, Total Reward: -641, Epsilon: 0.356 , loss: 116311.2578125\nEpisode 207/1000, Total Reward: -479, Epsilon: 0.354 , loss: 3885.327880859375\nEpisode 208/1000, Total Reward: -461, Epsilon: 0.353 , loss: 6294.142578125\nEpisode 209/1000, Total Reward: -596, Epsilon: 0.351 , loss: 6433.45751953125\nEpisode 210/1000, Total Reward: -659, Epsilon: 0.349 , loss: 2705.238037109375\nEpisode 211/1000, Total Reward: -443, Epsilon: 0.347 , loss: 8759.126953125\nEpisode 212/1000, Total Reward: -533, Epsilon: 0.346 , loss: 32387.744140625\nEpisode 213/1000, Total Reward: -434, Epsilon: 0.344 , loss: 10947.8984375\nEpisode 214/1000, Total Reward: -497, Epsilon: 0.342 , loss: 3749.923095703125\nEpisode 215/1000, Total Reward: -497, Epsilon: 0.340 , loss: 12295.58984375\nEpisode 216/1000, Total Reward: -407, Epsilon: 0.339 , loss: 791.1266479492188\nEpisode 217/1000, Total Reward: -677, Epsilon: 0.337 , loss: 15616.3857421875\nEpisode 218/1000, Total Reward: -434, Epsilon: 0.335 , loss: 1889.5531005859375\nEpisode 219/1000, Total Reward: -560, Epsilon: 0.334 , loss: 8250.1923828125\nEpisode 220/1000, Total Reward: -452, Epsilon: 0.332 , loss: 1712.51318359375\nEpisode 221/1000, Total Reward: -443, Epsilon: 0.330 , loss: 11904.2919921875\nEpisode 222/1000, Total Reward: -614, Epsilon: 0.329 , loss: 378.365966796875\nEpisode 223/1000, Total Reward: -587, Epsilon: 0.327 , loss: 428.1572265625\nEpisode 224/1000, Total Reward: -443, Epsilon: 0.325 , loss: 322.2518615722656\nEpisode 225/1000, Total Reward: -488, Epsilon: 0.324 , loss: 3805.542236328125\nEpisode 226/1000, Total Reward: -659, Epsilon: 0.322 , loss: 14123.6103515625\nEpisode 227/1000, Total Reward: -623, Epsilon: 0.321 , loss: 4709.3857421875\nEpisode 228/1000, Total Reward: -497, Epsilon: 0.319 , loss: 6622.41064453125\nEpisode 229/1000, Total Reward: -605, Epsilon: 0.317 , loss: 4209.55712890625\nEpisode 230/1000, Total Reward: -335, Epsilon: 0.316 , loss: 3236.412841796875\nEpisode 231/1000, Total Reward: -425, Epsilon: 0.314 , loss: 4960.357421875\nEpisode 232/1000, Total Reward: -551, Epsilon: 0.313 , loss: 1305.2437744140625\nEpisode 233/1000, Total Reward: -614, Epsilon: 0.311 , loss: 5956.55712890625\nEpisode 234/1000, Total Reward: -722, Epsilon: 0.309 , loss: 1685.857421875\nEpisode 235/1000, Total Reward: -722, Epsilon: 0.308 , loss: 2217.762451171875\nEpisode 236/1000, Total Reward: -497, Epsilon: 0.306 , loss: 5599.5859375\nEpisode 237/1000, Total Reward: -551, Epsilon: 0.305 , loss: 2564.179443359375\nEpisode 238/1000, Total Reward: -587, Epsilon: 0.303 , loss: 4327.16259765625\nEpisode 239/1000, Total Reward: -407, Epsilon: 0.302 , loss: 3156.169921875\nEpisode 240/1000, Total Reward: -524, Epsilon: 0.300 , loss: 8433.837890625\nEpisode 241/1000, Total Reward: -497, Epsilon: 0.299 , loss: 740.552001953125\nEpisode 242/1000, Total Reward: -371, Epsilon: 0.297 , loss: 5616.46533203125\nEpisode 243/1000, Total Reward: -569, Epsilon: 0.296 , loss: 2102.029052734375\nEpisode 244/1000, Total Reward: -650, Epsilon: 0.294 , loss: 26136.318359375\nEpisode 245/1000, Total Reward: -614, Epsilon: 0.293 , loss: 4729.59326171875\nEpisode 246/1000, Total Reward: -501, Epsilon: 0.291 , loss: 27630.193359375\nEpisode 247/1000, Total Reward: -668, Epsilon: 0.290 , loss: 1991.0576171875\nEpisode 248/1000, Total Reward: -596, Epsilon: 0.288 , loss: 337823.5625\nEpisode 249/1000, Total Reward: -767, Epsilon: 0.287 , loss: 45659.90234375\nEpisode 250/1000, Total Reward: -956, Epsilon: 0.286 , loss: 10228.6337890625\nEpisode 251/1000, Total Reward: -713, Epsilon: 0.284 , loss: 9248.9892578125\nEpisode 252/1000, Total Reward: -767, Epsilon: 0.283 , loss: 8873.2138671875\nEpisode 253/1000, Total Reward: -632, Epsilon: 0.281 , loss: 11863.2939453125\nEpisode 254/1000, Total Reward: -434, Epsilon: 0.280 , loss: 12528.80078125\nEpisode 255/1000, Total Reward: -542, Epsilon: 0.279 , loss: 7466.63916015625\nEpisode 256/1000, Total Reward: -506, Epsilon: 0.277 , loss: 5255.5927734375\nEpisode 257/1000, Total Reward: -416, Epsilon: 0.276 , loss: 6981.94140625\nEpisode 258/1000, Total Reward: -470, Epsilon: 0.274 , loss: 5453.47705078125\nEpisode 259/1000, Total Reward: -641, Epsilon: 0.273 , loss: 574.915283203125\nEpisode 260/1000, Total Reward: -803, Epsilon: 0.272 , loss: 846.7368774414062\nEpisode 261/1000, Total Reward: -353, Epsilon: 0.270 , loss: 7522.46923828125\nEpisode 262/1000, Total Reward: -407, Epsilon: 0.269 , loss: 598.647705078125\nEpisode 263/1000, Total Reward: -425, Epsilon: 0.268 , loss: 3500.68359375\nEpisode 264/1000, Total Reward: -542, Epsilon: 0.266 , loss: 3961.883544921875\nEpisode 265/1000, Total Reward: -416, Epsilon: 0.265 , loss: 3832.701171875\nEpisode 266/1000, Total Reward: -506, Epsilon: 0.264 , loss: 2266.795654296875\nEpisode 267/1000, Total Reward: -596, Epsilon: 0.262 , loss: 977.63037109375\nEpisode 268/1000, Total Reward: -731, Epsilon: 0.261 , loss: 19475.033203125\nEpisode 269/1000, Total Reward: -434, Epsilon: 0.260 , loss: 8578.0625\nEpisode 270/1000, Total Reward: -533, Epsilon: 0.258 , loss: 5529.43505859375\nEpisode 271/1000, Total Reward: -1082, Epsilon: 0.257 , loss: 4294.22705078125\nEpisode 272/1000, Total Reward: -398, Epsilon: 0.256 , loss: 2052.361083984375\nEpisode 273/1000, Total Reward: -488, Epsilon: 0.255 , loss: 4784.10791015625\nEpisode 274/1000, Total Reward: -650, Epsilon: 0.253 , loss: 4079.381103515625\nEpisode 275/1000, Total Reward: -506, Epsilon: 0.252 , loss: 11768.6728515625\nEpisode 276/1000, Total Reward: -407, Epsilon: 0.251 , loss: 4539.40869140625\nEpisode 277/1000, Total Reward: -641, Epsilon: 0.249 , loss: 1200.9669189453125\nEpisode 278/1000, Total Reward: -398, Epsilon: 0.248 , loss: 3391.8828125\nEpisode 279/1000, Total Reward: -425, Epsilon: 0.247 , loss: 2999.186767578125\nEpisode 280/1000, Total Reward: -731, Epsilon: 0.246 , loss: 2295.716064453125\nEpisode 281/1000, Total Reward: -443, Epsilon: 0.245 , loss: 3222.20703125\nEpisode 282/1000, Total Reward: -407, Epsilon: 0.243 , loss: 10761.3603515625\nEpisode 283/1000, Total Reward: -560, Epsilon: 0.242 , loss: 11859.1513671875\nEpisode 284/1000, Total Reward: -434, Epsilon: 0.241 , loss: 8779.7880859375\nEpisode 285/1000, Total Reward: -749, Epsilon: 0.240 , loss: 6700.51806640625\nEpisode 286/1000, Total Reward: -434, Epsilon: 0.238 , loss: 2835.25\nEpisode 287/1000, Total Reward: -416, Epsilon: 0.237 , loss: 3574.626220703125\n","output_type":"stream"}],"execution_count":null}]}