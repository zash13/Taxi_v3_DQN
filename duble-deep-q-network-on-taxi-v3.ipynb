{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-27T16:07:10.868457Z","iopub.execute_input":"2025-07-27T16:07:10.868698Z","iopub.status.idle":"2025-07-27T16:07:11.206914Z","shell.execute_reply.started":"2025-07-27T16:07:10.868664Z","shell.execute_reply":"2025-07-27T16:07:11.206077Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from os import stat\nimport numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras.optimizers import Adam\nimport random\nfrom tensorflow.keras.layers import Dense, Input\nfrom collections import deque\nfrom enum import Enum\n\n\nclass EpsilonPolicyType(Enum):\n    NONE = 0\n    DECAY = 1\n    SOFTLINEAR = 2\n\n\nclass RewardPolicyType(Enum):\n    NONE = 0\n    ERM = 1\n\n\nclass UpdateTargetNetworkType(Enum):\n    HARD = 0\n    SOFT = 1\n\n\nINFINITY = float(\"inf\")\n\n\nclass ExperienceBuffer:\n    def __init__(\n        self,\n        rewarder: \"RewardHelper\",\n        buffer_size=2000,\n        prefer_lower_heuristic=True,\n    ) -> None:\n        self.memory_buffer = deque(maxlen=buffer_size)\n        self.buffer_size = buffer_size\n        self.reward_helper = rewarder\n        self.prefer_lower_heuristic = prefer_lower_heuristic\n\n    def store_experience(\n        self, current_state, next_state, imm_reward, action, done, heuristic=0\n    ):\n        imm_reward = self.reward_helper.findReward(\n            current_state, imm_reward, heuristic, self.prefer_lower_heuristic\n        )\n        self.memory_buffer.append(\n            {\n                \"current_state\": current_state,\n                \"action\": action,\n                \"reward\": imm_reward,\n                \"next_state\": next_state,\n                \"heuristic\": heuristic,\n                \"done\": done,\n            }\n        )\n\n    def sample_batch(self, count):\n        if len(self.memory_buffer) < count:\n            return None\n        batch = random.sample(self.memory_buffer, count)\n        states = np.vstack([item[\"current_state\"] for item in batch])\n        next_states = np.vstack([item[\"next_state\"] for item in batch])\n        rewards = np.array([item[\"reward\"] for item in batch])\n        actions = np.array([item[\"action\"] for item in batch])\n        dones = np.array([item[\"done\"] for item in batch])\n        heuristics = np.array([item[\"heuristic\"] for item in batch])\n        return states, next_states, rewards, actions, dones, heuristics\n\n\nclass QNetwork:\n    def __init__(self, state_size, action_size, learning_rate=0.001) -> None:\n        self.state_size = state_size\n        self.action_size = action_size\n        self._model = self._initiate_model()\n        self._model.compile(loss=\"mse\", optimizer=Adam(learning_rate=learning_rate))\n\n    def _initiate_model(self):\n        return keras.Sequential(\n            [\n                Input(shape=(self.state_size,)),\n                Dense(units=64, activation=\"relu\"),\n                Dense(units=32, activation=\"relu\"),\n                Dense(units=32, activation=\"relu\"),\n                Dense(units=self.action_size, activation=\"linear\"),\n            ]\n        )\n\n    def predict(self, states, verbose=0):\n        return self._model.predict(states, verbose=verbose)\n\n    def fit(self, states, q_targets, epochs=1, verbose=0):\n        history = self._model.fit(states, q_targets, epochs=epochs, verbose=verbose)\n        return history.history[\"loss\"][0]\n\n    def set_weights(self, weights):\n        self._model.set_weights(weights)\n        return True\n\n    def get_weights(self):\n        return self._model.get_weights()\n\n\n# we learnd that agent should balance between exploration and exploitation\n# without this section , it seems that agent just try to explor new path\n#\nclass EpsilonPolicy:\n    def __init__(\n        self,\n        epsilon_min: float = 0.01,\n        epsilon_decay: float = 0.995,\n        policy: EpsilonPolicyType = EpsilonPolicyType.DECAY,\n        update_per_episod: bool = True,\n    ):\n        self.policy = policy\n        self.visited_states = {}\n        self.epsilon: float = INFINITY\n        self.epsilon_min = epsilon_min\n        self.epsilon_decay = epsilon_decay\n        self.update_per_episod = update_per_episod\n        self.previous_episode = 0\n\n    def adjust_epsilon(\n        self,\n        epsilon,\n        episode_count,\n        max_episodes=100,\n    ):\n        self.epsilon = epsilon\n        if self.update_per_episod:\n            if self.previous_episode == episode_count:\n                return self.epsilon\n            else:\n                self.previous_episode = episode_count\n        if self.policy == EpsilonPolicyType.DECAY:\n            return self.linear_decay(episode_count, max_episodes)\n        elif self.policy == EpsilonPolicyType.SOFTLINEAR:\n            return self.soft_linear_decay(episode_count, max_episodes)\n        else:\n            return self.epsilon\n\n    def linear_decay(self, episode_count, max_episodes):\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n        self.epsilon = max(self.epsilon, self.epsilon_min)\n        return self.epsilon\n\n    def soft_linear_decay(self, episode_count, max_episodes=200):\n        if self.epsilon > self.epsilon_min:\n            target_epsilon = max(\n                self.epsilon_min,\n                1.0 - (1.0 - self.epsilon_min) * (episode_count / max_episodes),\n            )\n            self.epsilon = self.epsilon * self.epsilon_decay + target_epsilon * (\n                1 - self.epsilon_decay\n            )\n        return self.epsilon\n\n\nclass RewardHelper:\n    def __init__(\n        self,\n        progress_bonus: float = 0.05,\n        exploration_bonus: float = 0.1,\n        policy: RewardPolicyType = RewardPolicyType.ERM,\n    ):\n        self.progress_bonus = progress_bonus\n        self.exploration_bonus = exploration_bonus\n        self.policy = policy\n        self.old_huristic = INFINITY\n        self.visited_states = {}\n\n    def findReward(self, state, reward, heuristic=None, lowerHuristicBetter=True):\n        if self.policy == RewardPolicyType.ERM:\n            return self._ERM(state, reward, heuristic, lowerHuristicBetter)\n        elif self.policy == RewardPolicyType.NONE:\n            return self._none(reward)\n        else:\n            return reward\n\n    def _none(self, reward):\n        return reward\n\n    def _ERM(self, state, reward, heuristic=None, lowerHuristicBetter=True):\n        state_key = tuple(state.flatten()) if state is not None else None\n        is_new_state = state_key is not None and state_key not in self.visited_states\n        if is_new_state:\n            if is_new_state and state_key is not None:\n                self.visited_states[state_key] = (\n                    heuristic if heuristic is not None else INFINITY\n                )\n\n        progress = 0.0\n        if heuristic is not None and state_key is not None:\n            old_heuristic = self.visited_states[state_key]\n            if (\n                old_heuristic != INFINITY\n                and (heuristic < old_heuristic and lowerHuristicBetter)\n                or (heuristic > old_heuristic and not lowerHuristicBetter)\n            ):\n                progress = self.progress_bonus\n                self.visited_states[state_key] = heuristic\n\n        new_reward = reward\n        if is_new_state:\n            new_reward += self.exploration_bonus\n        if progress > 0:\n            new_reward += self.progress_bonus\n        return new_reward\n\n\nclass DQNAgent:\n    def __init__(\n        self,\n        action_size,\n        state_size,\n        learning_rate=0.001,\n        buffer_size=2000,\n        batch_size=32,\n        gamma=0.99,\n        max_episodes=200,\n        epsilon=1.0,\n        epsilon_min=0.01,\n        epsilon_decay=0.995,\n        epsilon_policy: EpsilonPolicy = None,\n        reward_policy: RewardPolicyType = RewardPolicyType.NONE,\n        prefer_lower_heuristic=True,\n        progress_bonus: float = 0.05,\n        exploration_bonus: float = 0.1,\n    ) -> None:\n        self.action_size = action_size\n        self.state_size = state_size\n        self.gamma = gamma\n        self.batch_size = batch_size\n        self.epsilon = epsilon\n        self.episode_count = 0\n        self.max_episodes = max_episodes\n\n        self.model = QNetwork(state_size, action_size, learning_rate)\n        self.epsilon_policy = epsilon_policy or EpsilonPolicy(\n            epsilon_min=epsilon_min,\n            epsilon_decay=epsilon_decay,\n            policy=EpsilonPolicyType.DECAY,\n        )\n        self.buffer_helper = ExperienceBuffer(\n            RewardHelper(progress_bonus, exploration_bonus, reward_policy),\n            buffer_size=buffer_size,\n            prefer_lower_heuristic=prefer_lower_heuristic,\n        )\n\n    def train(self, episode):\n        data = self.buffer_helper.sample_batch(self.batch_size)\n        if data is None:\n            return None\n        states, next_states, rewards, actions, dones, heuristics = data\n        q_current = self.model.predict(states, verbose=0)\n        q_next = self.model.predict(next_states, verbose=0)\n        q_targets = q_current.copy()\n        for i in range(self.batch_size):\n            if not dones[i]:\n                q_targets[i, actions[i]] = rewards[i] + self.gamma * np.max(q_next[i])\n            else:\n                q_targets[i, actions[i]] = rewards[i]\n            q_targets[i, actions[i]] = np.clip(q_targets[i, actions[i]], -10, 10)\n\n        self._update_exploration_rate(episode_count=episode)\n        loss = self.model.fit(states, q_targets, epochs=1, verbose=0)\n        return loss\n\n    def _update_exploration_rate(self, episode_count):\n        self.epsilon = self.epsilon_policy.adjust_epsilon(\n            self.epsilon, episode_count=episode_count, max_episodes=self.max_episodes\n        )\n\n    def select_action(self, current_state):\n        if np.random.uniform(0, 1) < self.epsilon:\n            return np.random.choice(self.action_size)\n        else:\n            q_value = self.model.predict(current_state, verbose=0)[0]\n            return np.argmax(q_value)\n\n\nclass DoubleDQNAgent(DQNAgent):\n    def __init__(\n        self,\n        action_size,\n        state_size,\n        learning_rate=0.001,\n        buffer_size=2000,\n        batch_size=32,\n        gamma=0.99,\n        max_episodes=200,\n        epsilon=1.0,\n        epsilon_min=0.01,\n        epsilon_decay=0.995,\n        epsilon_policy: EpsilonPolicy = None,\n        reward_policy: RewardPolicyType = RewardPolicyType.NONE,\n        prefer_lower_heuristic=True,\n        progress_bonus: float = 0.05,\n        exploration_bonus: float = 0.1,\n        update_target_network_method: UpdateTargetNetworkType = UpdateTargetNetworkType.HARD,\n        update_factor=0.005,\n        target_update_frequency=10,\n    ) -> None:\n        super().__init__(\n            action_size,\n            state_size,\n            learning_rate,\n            buffer_size,\n            batch_size,\n            gamma,\n            max_episodes,\n            epsilon,\n            epsilon_min,\n            epsilon_decay,\n            epsilon_policy,\n            reward_policy,\n            prefer_lower_heuristic,\n            progress_bonus,\n            exploration_bonus,\n        )\n        self.update_target_network_method = update_target_network_method\n        self.online_model = self.model\n        self.target_model = QNetwork(state_size, action_size, learning_rate)\n        self.previous_episode = 0\n        self.update_factor = update_factor\n        self.target_model.set_weights(self.online_model.get_weights())\n        self.target_update_frequency = target_update_frequency\n\n    def train(self, episode):\n        data = self.buffer_helper.sample_batch(self.batch_size)\n        if data is None:\n            return None\n        states, next_states, rewards, actions, dones, heuristics = data\n\n        # Here's what I understand is happening:\n        # We have two models: one acts based on Temporal Difference (TD) learning, and the other (called the target network) behaves more like a Monte Carlo method.\n        # Basically, one model is updated frequently (every epoch), while the other is updated less often (at a fixed interval).\n        # The frequently updated model is used for action selection, while the target network is used to evaluate Q-values.\n        # The TD update equation: Q(s, a) = r + γ * (r + max_a' Q(s', a') - Q(s, a)) becomes:\n        # Q_target(s, a) = r + γ * Q_target(s', argmax_a' Q_online(s', a'))\n        q_current = self.online_model.predict(states, verbose=0)\n        q_next_online = self.online_model.predict(next_states, verbose=0)\n        q_next_target = self.target_model.predict(next_states, verbose=0)\n        q_targets = q_current.copy()\n        for i in range(self.batch_size):\n            if not dones[i]:\n                best_action = np.argmax(q_next_online[i])\n                q_targets[i, actions[i]] = (\n                    rewards[i] + self.gamma * q_next_target[i, best_action]\n                )\n            else:\n                q_targets[i, actions[i]] = rewards[i]\n            #q_targets[i, actions[i]] = np.clip(q_targets[i, actions[i]], -10, 10)\n\n        # and here is where target network update base on frequency\n        if episode % self.target_update_frequency == 0:\n            self._update_target_network()\n        self._update_exploration_rate(episode_count=episode)\n        loss = self.online_model.fit(states, q_targets, epochs=1, verbose=0)\n        return loss\n\n    def _update_target_network(self):\n        if self.update_target_network_method == UpdateTargetNetworkType.HARD:\n            self.target_model.set_weights(self.online_model.get_weights())\n        elif self.update_target_network_method == UpdateTargetNetworkType.SOFT:\n            online_weights = self.online_model.get_weights()\n            target_weights = self.target_model.get_weights()\n            updated_weights = []\n            for online_w, target_w in zip(online_weights, target_weights):\n                updated_weights.append(\n                    self.update_factor * online_w + (1 - self.update_factor) * target_w\n                )\n            self.target_model.set_weights(updated_weights)\n\n    def select_action(self, current_state):\n        if np.random.uniform(0, 1) < self.epsilon:\n            return np.random.choice(self.action_size)\n        else:\n            q_value = self.online_model.predict(current_state, verbose=0)[0]\n            return np.argmax(q_value)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T16:07:11.208634Z","iopub.execute_input":"2025-07-27T16:07:11.208993Z","iopub.status.idle":"2025-07-27T16:07:29.089923Z","shell.execute_reply.started":"2025-07-27T16:07:11.208972Z","shell.execute_reply":"2025-07-27T16:07:29.089127Z"}},"outputs":[{"name":"stderr","text":"2025-07-27 16:07:13.243927: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753632433.499249      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753632433.571749      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import gymnasium as gym\nimport numpy as np\n\nenv = gym.make(\"Taxi-v3\")\n\nstate_size = env.observation_space.n  # 500\naction_size = env.action_space.n  # 6\n\nepsilon_min = 0.01\nepsilon_decay = 0.995\nep_policy = EpsilonPolicy(\n    epsilon_min=epsilon_min,\n    epsilon_decay=epsilon_decay,\n    policy=EpsilonPolicyType.DECAY,\n)\n\nnum_episodes = 1000\nagent = DoubleDQNAgent(\n    action_size=action_size,\n    state_size=state_size,\n    learning_rate=0.001,\n    buffer_size=2000,\n    batch_size=96,\n    gamma=0.99,\n    max_episodes=num_episodes,\n    epsilon=1.0,\n    epsilon_min=epsilon_min,\n    epsilon_decay=epsilon_decay,\n    epsilon_policy=ep_policy,\n    reward_policy=RewardPolicyType.ERM,\n    prefer_lower_heuristic=False,\n    progress_bonus=0.05,\n    exploration_bonus=0.1,\n    update_target_network_method=UpdateTargetNetworkType.HARD,\n    update_factor=0.005,\n    target_update_frequency=10,\n)\n\n# Training loop\nrewards = []\nmax_steps = 200\ncounter = 0\n\nfor episode in range(num_episodes):\n    state, _ = env.reset()\n    state = np.eye(state_size)[state]\n    total_reward = 0\n    done = False\n    step = 0\n\n    while not done and step < max_steps:\n        counter += 1\n        action = agent.select_action(np.array([state]))\n\n        next_state, reward, terminated, truncated, _ = env.step(action)\n        next_state = np.eye(state_size)[next_state]\n        done = terminated or truncated\n\n        agent.buffer_helper.store_experience(\n            current_state=state,\n            next_state=next_state,\n            imm_reward=reward,\n            action=action,\n            done=done,\n            heuristic=None,\n        )\n\n        loss = agent.train(episode)\n\n        state = next_state\n        total_reward += reward\n        step += 1\n\n    rewards.append(total_reward)\n    print(\n        f\"Episode {episode}/{num_episodes}, Total Reward: {total_reward}, Epsilon: {agent.epsilon:.3f} , loss: {loss}\"\n    )\n\n\nenv.close()\n\nimport matplotlib.pyplot as plt\n\nplt.plot(rewards)\nplt.xlabel(\"Episode\")\nplt.ylabel(\"Total Reward\")\nplt.title(\"Training Progress on Taxi-v3 with Double DQN\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T16:24:56.731492Z","iopub.execute_input":"2025-07-27T16:24:56.732038Z"}},"outputs":[{"name":"stdout","text":"Episode 0/1000, Total Reward: -713, Epsilon: 1.000 , loss: 0.04620656371116638\nEpisode 1/1000, Total Reward: -785, Epsilon: 0.995 , loss: 0.0038964198902249336\nEpisode 2/1000, Total Reward: -722, Epsilon: 0.990 , loss: 0.11782434582710266\nEpisode 3/1000, Total Reward: -902, Epsilon: 0.985 , loss: 0.12984992563724518\nEpisode 4/1000, Total Reward: -812, Epsilon: 0.980 , loss: 0.276673287153244\nEpisode 5/1000, Total Reward: -731, Epsilon: 0.975 , loss: 0.04668305814266205\nEpisode 6/1000, Total Reward: -794, Epsilon: 0.970 , loss: 0.018176669254899025\nEpisode 7/1000, Total Reward: -704, Epsilon: 0.966 , loss: 0.008343305438756943\nEpisode 8/1000, Total Reward: -794, Epsilon: 0.961 , loss: 0.0886470377445221\nEpisode 9/1000, Total Reward: -713, Epsilon: 0.956 , loss: 0.017544759437441826\nEpisode 10/1000, Total Reward: -785, Epsilon: 0.951 , loss: 0.04337209463119507\nEpisode 11/1000, Total Reward: -812, Epsilon: 0.946 , loss: 0.08072136342525482\nEpisode 12/1000, Total Reward: -830, Epsilon: 0.942 , loss: 0.016573814675211906\nEpisode 13/1000, Total Reward: -686, Epsilon: 0.937 , loss: 0.040540002286434174\nEpisode 14/1000, Total Reward: -677, Epsilon: 0.932 , loss: 0.03649420663714409\nEpisode 15/1000, Total Reward: -821, Epsilon: 0.928 , loss: 0.7401034832000732\nEpisode 16/1000, Total Reward: -28, Epsilon: 0.923 , loss: 0.4999994933605194\nEpisode 17/1000, Total Reward: -830, Epsilon: 0.918 , loss: 0.023226672783493996\nEpisode 18/1000, Total Reward: -686, Epsilon: 0.914 , loss: 0.02340276539325714\nEpisode 19/1000, Total Reward: -722, Epsilon: 0.909 , loss: 0.04687434434890747\nEpisode 20/1000, Total Reward: -884, Epsilon: 0.905 , loss: 0.10056987404823303\nEpisode 21/1000, Total Reward: -722, Epsilon: 0.900 , loss: 0.01559759397059679\nEpisode 22/1000, Total Reward: -339, Epsilon: 0.896 , loss: 0.10271633416414261\nEpisode 23/1000, Total Reward: -758, Epsilon: 0.891 , loss: 0.01509395707398653\nEpisode 24/1000, Total Reward: -686, Epsilon: 0.887 , loss: 0.12042204290628433\nEpisode 25/1000, Total Reward: -767, Epsilon: 0.882 , loss: 0.012785005383193493\nEpisode 26/1000, Total Reward: -767, Epsilon: 0.878 , loss: 0.040923308581113815\nEpisode 27/1000, Total Reward: -650, Epsilon: 0.873 , loss: 0.01233187597244978\nEpisode 28/1000, Total Reward: -803, Epsilon: 0.869 , loss: 0.01290657464414835\nEpisode 29/1000, Total Reward: -767, Epsilon: 0.865 , loss: 0.025485897436738014\nEpisode 30/1000, Total Reward: -713, Epsilon: 0.860 , loss: 0.20794044435024261\nEpisode 31/1000, Total Reward: -677, Epsilon: 0.856 , loss: 0.17415587604045868\nEpisode 32/1000, Total Reward: -722, Epsilon: 0.852 , loss: 0.023381998762488365\nEpisode 33/1000, Total Reward: -686, Epsilon: 0.848 , loss: 0.08409134298563004\nEpisode 34/1000, Total Reward: -614, Epsilon: 0.843 , loss: 0.05279064178466797\nEpisode 35/1000, Total Reward: -704, Epsilon: 0.839 , loss: 0.04013076424598694\nEpisode 36/1000, Total Reward: -641, Epsilon: 0.835 , loss: 0.04057065024971962\nEpisode 37/1000, Total Reward: -507, Epsilon: 0.831 , loss: 0.04230692982673645\nEpisode 38/1000, Total Reward: -695, Epsilon: 0.827 , loss: 0.04803379252552986\nEpisode 39/1000, Total Reward: -731, Epsilon: 0.822 , loss: 0.06028705835342407\nEpisode 40/1000, Total Reward: -322, Epsilon: 0.818 , loss: 0.16696476936340332\nEpisode 41/1000, Total Reward: -596, Epsilon: 0.814 , loss: 0.370032399892807\nEpisode 42/1000, Total Reward: -794, Epsilon: 0.810 , loss: 0.06437960267066956\nEpisode 43/1000, Total Reward: -686, Epsilon: 0.806 , loss: 0.030639274045825005\nEpisode 44/1000, Total Reward: -713, Epsilon: 0.802 , loss: 0.011830194853246212\nEpisode 45/1000, Total Reward: -578, Epsilon: 0.798 , loss: 0.1684541553258896\nEpisode 46/1000, Total Reward: -641, Epsilon: 0.794 , loss: 0.08045875281095505\nEpisode 47/1000, Total Reward: -551, Epsilon: 0.790 , loss: 0.20859907567501068\nEpisode 48/1000, Total Reward: -668, Epsilon: 0.786 , loss: 0.12388380616903305\nEpisode 49/1000, Total Reward: -614, Epsilon: 0.782 , loss: 0.0429740846157074\nEpisode 50/1000, Total Reward: -695, Epsilon: 0.778 , loss: 0.05493275821208954\nEpisode 51/1000, Total Reward: -677, Epsilon: 0.774 , loss: 0.4361248016357422\nEpisode 52/1000, Total Reward: -203, Epsilon: 0.771 , loss: 0.028367385268211365\nEpisode 53/1000, Total Reward: -740, Epsilon: 0.767 , loss: 0.2726288437843323\nEpisode 54/1000, Total Reward: -363, Epsilon: 0.763 , loss: 0.06892591714859009\nEpisode 55/1000, Total Reward: -632, Epsilon: 0.759 , loss: 0.18487770855426788\nEpisode 56/1000, Total Reward: -386, Epsilon: 0.755 , loss: 0.15101225674152374\nEpisode 57/1000, Total Reward: -14, Epsilon: 0.751 , loss: 0.0668659433722496\nEpisode 58/1000, Total Reward: -614, Epsilon: 0.748 , loss: 0.012917323969304562\nEpisode 59/1000, Total Reward: -276, Epsilon: 0.744 , loss: 0.04577941074967384\nEpisode 60/1000, Total Reward: -677, Epsilon: 0.740 , loss: 0.2109883576631546\nEpisode 61/1000, Total Reward: -605, Epsilon: 0.737 , loss: 0.07603773474693298\nEpisode 62/1000, Total Reward: -659, Epsilon: 0.733 , loss: 0.15520380437374115\nEpisode 63/1000, Total Reward: -641, Epsilon: 0.729 , loss: 0.03524702787399292\nEpisode 64/1000, Total Reward: -569, Epsilon: 0.726 , loss: 0.05780626833438873\nEpisode 65/1000, Total Reward: -605, Epsilon: 0.722 , loss: 0.13695012032985687\nEpisode 66/1000, Total Reward: -596, Epsilon: 0.718 , loss: 0.08213160932064056\nEpisode 67/1000, Total Reward: -605, Epsilon: 0.715 , loss: 0.021435409784317017\nEpisode 68/1000, Total Reward: -623, Epsilon: 0.711 , loss: 0.1399141252040863\nEpisode 69/1000, Total Reward: -641, Epsilon: 0.708 , loss: 0.7556650638580322\nEpisode 70/1000, Total Reward: -524, Epsilon: 0.704 , loss: 0.09387993812561035\nEpisode 71/1000, Total Reward: -641, Epsilon: 0.701 , loss: 0.05239051207900047\nEpisode 72/1000, Total Reward: -650, Epsilon: 0.697 , loss: 0.010289271362125874\nEpisode 73/1000, Total Reward: -479, Epsilon: 0.694 , loss: 0.6989635825157166\nEpisode 74/1000, Total Reward: -596, Epsilon: 0.690 , loss: 0.02709236927330494\nEpisode 75/1000, Total Reward: -524, Epsilon: 0.687 , loss: 0.06324366480112076\nEpisode 76/1000, Total Reward: -578, Epsilon: 0.683 , loss: 0.01624158024787903\nEpisode 77/1000, Total Reward: -614, Epsilon: 0.680 , loss: 0.7139536738395691\nEpisode 78/1000, Total Reward: -515, Epsilon: 0.676 , loss: 0.7020041346549988\nEpisode 79/1000, Total Reward: -515, Epsilon: 0.673 , loss: 0.0459010936319828\nEpisode 80/1000, Total Reward: -174, Epsilon: 0.670 , loss: 0.4137181341648102\nEpisode 81/1000, Total Reward: -614, Epsilon: 0.666 , loss: 0.053639721125364304\nEpisode 82/1000, Total Reward: -641, Epsilon: 0.663 , loss: 0.0491810142993927\nEpisode 83/1000, Total Reward: -198, Epsilon: 0.660 , loss: 0.3175137937068939\nEpisode 84/1000, Total Reward: -130, Epsilon: 0.656 , loss: 0.13409587740898132\nEpisode 85/1000, Total Reward: -524, Epsilon: 0.653 , loss: 0.22048504650592804\nEpisode 86/1000, Total Reward: -614, Epsilon: 0.650 , loss: 0.10077764838933945\nEpisode 87/1000, Total Reward: -569, Epsilon: 0.647 , loss: 0.0544450469315052\nEpisode 88/1000, Total Reward: -560, Epsilon: 0.643 , loss: 0.041172295808792114\nEpisode 89/1000, Total Reward: -287, Epsilon: 0.640 , loss: 0.07691933959722519\nEpisode 90/1000, Total Reward: -488, Epsilon: 0.637 , loss: 0.17401139438152313\nEpisode 91/1000, Total Reward: -465, Epsilon: 0.634 , loss: 0.2387046068906784\nEpisode 92/1000, Total Reward: -605, Epsilon: 0.631 , loss: 0.10036492347717285\nEpisode 93/1000, Total Reward: -120, Epsilon: 0.627 , loss: 0.3006385862827301\nEpisode 94/1000, Total Reward: -677, Epsilon: 0.624 , loss: 0.014555382542312145\nEpisode 95/1000, Total Reward: -560, Epsilon: 0.621 , loss: 0.5268614888191223\nEpisode 96/1000, Total Reward: -133, Epsilon: 0.618 , loss: 0.19203583896160126\nEpisode 97/1000, Total Reward: -13, Epsilon: 0.615 , loss: 0.17013974487781525\nEpisode 98/1000, Total Reward: -542, Epsilon: 0.612 , loss: 0.05245310068130493\nEpisode 99/1000, Total Reward: -112, Epsilon: 0.609 , loss: 0.1936195343732834\nEpisode 100/1000, Total Reward: -524, Epsilon: 0.606 , loss: 0.12516078352928162\nEpisode 101/1000, Total Reward: -641, Epsilon: 0.603 , loss: 0.700734555721283\nEpisode 102/1000, Total Reward: -515, Epsilon: 0.600 , loss: 0.040685657411813736\nEpisode 103/1000, Total Reward: -159, Epsilon: 0.597 , loss: 0.07264862954616547\nEpisode 104/1000, Total Reward: -50, Epsilon: 0.594 , loss: 0.1257418692111969\nEpisode 105/1000, Total Reward: -244, Epsilon: 0.591 , loss: 0.23507551848888397\nEpisode 106/1000, Total Reward: -434, Epsilon: 0.588 , loss: 0.08315258473157883\nEpisode 107/1000, Total Reward: -497, Epsilon: 0.585 , loss: 0.014119669795036316\nEpisode 108/1000, Total Reward: -596, Epsilon: 0.582 , loss: 0.058191295713186264\nEpisode 109/1000, Total Reward: -524, Epsilon: 0.579 , loss: 0.044433027505874634\nEpisode 110/1000, Total Reward: -375, Epsilon: 0.576 , loss: 0.07620865851640701\nEpisode 111/1000, Total Reward: -150, Epsilon: 0.573 , loss: 0.3996903598308563\nEpisode 112/1000, Total Reward: 9, Epsilon: 0.570 , loss: 0.12277296185493469\nEpisode 113/1000, Total Reward: -497, Epsilon: 0.568 , loss: 0.048371005803346634\nEpisode 114/1000, Total Reward: -232, Epsilon: 0.565 , loss: 0.016430556774139404\nEpisode 115/1000, Total Reward: -97, Epsilon: 0.562 , loss: 0.03416812792420387\nEpisode 116/1000, Total Reward: -136, Epsilon: 0.559 , loss: 0.02619272656738758\nEpisode 117/1000, Total Reward: -524, Epsilon: 0.556 , loss: 0.19565802812576294\nEpisode 118/1000, Total Reward: -78, Epsilon: 0.554 , loss: 0.024607712402939796\nEpisode 119/1000, Total Reward: -144, Epsilon: 0.551 , loss: 0.04853842779994011\nEpisode 120/1000, Total Reward: -390, Epsilon: 0.548 , loss: 0.03320613130927086\nEpisode 121/1000, Total Reward: -560, Epsilon: 0.545 , loss: 0.05359426513314247\nEpisode 122/1000, Total Reward: -24, Epsilon: 0.543 , loss: 0.0064021828584373\nEpisode 123/1000, Total Reward: -533, Epsilon: 0.540 , loss: 0.003975357860326767\nEpisode 124/1000, Total Reward: -253, Epsilon: 0.537 , loss: 0.019384058192372322\nEpisode 125/1000, Total Reward: -533, Epsilon: 0.534 , loss: 0.0776805505156517\nEpisode 126/1000, Total Reward: -98, Epsilon: 0.532 , loss: 0.03482954204082489\nEpisode 127/1000, Total Reward: -39, Epsilon: 0.529 , loss: 0.09041458368301392\nEpisode 128/1000, Total Reward: -66, Epsilon: 0.526 , loss: 0.13293904066085815\nEpisode 129/1000, Total Reward: -479, Epsilon: 0.524 , loss: 0.02836366929113865\nEpisode 130/1000, Total Reward: -19, Epsilon: 0.521 , loss: 0.254286527633667\nEpisode 131/1000, Total Reward: -256, Epsilon: 0.519 , loss: 0.029492385685443878\nEpisode 132/1000, Total Reward: -4, Epsilon: 0.516 , loss: 0.05323187634348869\nEpisode 133/1000, Total Reward: -22, Epsilon: 0.513 , loss: 0.09716400504112244\nEpisode 134/1000, Total Reward: -452, Epsilon: 0.511 , loss: 0.040213197469711304\nEpisode 135/1000, Total Reward: -461, Epsilon: 0.508 , loss: 0.023361729457974434\nEpisode 136/1000, Total Reward: -461, Epsilon: 0.506 , loss: 0.031119326129555702\nEpisode 137/1000, Total Reward: -4, Epsilon: 0.503 , loss: 0.14384576678276062\nEpisode 138/1000, Total Reward: -34, Epsilon: 0.501 , loss: 0.056005608290433884\nEpisode 139/1000, Total Reward: -112, Epsilon: 0.498 , loss: 0.3282836377620697\nEpisode 140/1000, Total Reward: -166, Epsilon: 0.496 , loss: 0.0830606147646904\nEpisode 141/1000, Total Reward: -75, Epsilon: 0.493 , loss: 0.06941547244787216\nEpisode 142/1000, Total Reward: -497, Epsilon: 0.491 , loss: 0.046549487859010696\nEpisode 143/1000, Total Reward: -37, Epsilon: 0.488 , loss: 0.05272573605179787\nEpisode 144/1000, Total Reward: -100, Epsilon: 0.486 , loss: 0.21634197235107422\nEpisode 145/1000, Total Reward: -524, Epsilon: 0.483 , loss: 0.02631293796002865\nEpisode 146/1000, Total Reward: -488, Epsilon: 0.481 , loss: 0.056021060794591904\nEpisode 147/1000, Total Reward: -170, Epsilon: 0.479 , loss: 0.052520688623189926\nEpisode 148/1000, Total Reward: -226, Epsilon: 0.476 , loss: 0.004909707698971033\nEpisode 149/1000, Total Reward: -434, Epsilon: 0.474 , loss: 0.026100538671016693\nEpisode 150/1000, Total Reward: -86, Epsilon: 0.471 , loss: 0.11866287142038345\nEpisode 151/1000, Total Reward: -434, Epsilon: 0.469 , loss: 0.01124553382396698\nEpisode 152/1000, Total Reward: -5, Epsilon: 0.467 , loss: 0.031564559787511826\nEpisode 153/1000, Total Reward: -443, Epsilon: 0.464 , loss: 0.04467909038066864\nEpisode 154/1000, Total Reward: -59, Epsilon: 0.462 , loss: 0.05495757237076759\nEpisode 155/1000, Total Reward: -143, Epsilon: 0.460 , loss: 0.04893244430422783\nEpisode 156/1000, Total Reward: -488, Epsilon: 0.458 , loss: 0.21015208959579468\nEpisode 157/1000, Total Reward: -42, Epsilon: 0.455 , loss: 0.021868044510483742\nEpisode 158/1000, Total Reward: -524, Epsilon: 0.453 , loss: 0.014941818080842495\nEpisode 159/1000, Total Reward: -182, Epsilon: 0.451 , loss: 0.015951810404658318\nEpisode 160/1000, Total Reward: -533, Epsilon: 0.448 , loss: 0.05877499654889107\nEpisode 161/1000, Total Reward: -506, Epsilon: 0.446 , loss: 0.14806948602199554\nEpisode 162/1000, Total Reward: -515, Epsilon: 0.444 , loss: 0.9102943539619446\nEpisode 163/1000, Total Reward: -452, Epsilon: 0.442 , loss: 0.015247642993927002\nEpisode 164/1000, Total Reward: -45, Epsilon: 0.440 , loss: 0.05299999937415123\nEpisode 165/1000, Total Reward: -497, Epsilon: 0.437 , loss: 0.170374795794487\nEpisode 166/1000, Total Reward: -434, Epsilon: 0.435 , loss: 0.03585771843791008\nEpisode 167/1000, Total Reward: -31, Epsilon: 0.433 , loss: 0.0127679742872715\nEpisode 168/1000, Total Reward: -128, Epsilon: 0.431 , loss: 0.02264680527150631\nEpisode 169/1000, Total Reward: -268, Epsilon: 0.429 , loss: 0.47707056999206543\nEpisode 170/1000, Total Reward: -434, Epsilon: 0.427 , loss: 0.4462505578994751\nEpisode 171/1000, Total Reward: -101, Epsilon: 0.424 , loss: 0.21644850075244904\nEpisode 172/1000, Total Reward: -86, Epsilon: 0.422 , loss: 0.024364754557609558\nEpisode 173/1000, Total Reward: -470, Epsilon: 0.420 , loss: 0.4429464638233185\nEpisode 174/1000, Total Reward: -46, Epsilon: 0.418 , loss: 0.4118255376815796\nEpisode 175/1000, Total Reward: -88, Epsilon: 0.416 , loss: 0.04683178663253784\nEpisode 176/1000, Total Reward: -166, Epsilon: 0.414 , loss: 0.20316702127456665\nEpisode 177/1000, Total Reward: -114, Epsilon: 0.412 , loss: 0.2213553935289383\nEpisode 178/1000, Total Reward: -2, Epsilon: 0.410 , loss: 0.2356387823820114\nEpisode 179/1000, Total Reward: -358, Epsilon: 0.408 , loss: 0.057924360036849976\nEpisode 180/1000, Total Reward: -174, Epsilon: 0.406 , loss: 0.16952796280384064\nEpisode 181/1000, Total Reward: -398, Epsilon: 0.404 , loss: 0.5436634421348572\nEpisode 182/1000, Total Reward: -88, Epsilon: 0.402 , loss: 0.03313465788960457\nEpisode 183/1000, Total Reward: -398, Epsilon: 0.400 , loss: 0.10404789447784424\nEpisode 184/1000, Total Reward: -470, Epsilon: 0.398 , loss: 0.19398576021194458\nEpisode 185/1000, Total Reward: -143, Epsilon: 0.396 , loss: 0.10901603102684021\nEpisode 186/1000, Total Reward: -308, Epsilon: 0.394 , loss: 0.18442268669605255\nEpisode 187/1000, Total Reward: -452, Epsilon: 0.392 , loss: 0.07258812338113785\nEpisode 188/1000, Total Reward: -425, Epsilon: 0.390 , loss: 0.21454954147338867\nEpisode 189/1000, Total Reward: -174, Epsilon: 0.388 , loss: 0.09725586324930191\nEpisode 190/1000, Total Reward: -76, Epsilon: 0.386 , loss: 0.25088176131248474\nEpisode 191/1000, Total Reward: -2, Epsilon: 0.384 , loss: 0.2833081781864166\nEpisode 192/1000, Total Reward: -60, Epsilon: 0.382 , loss: 0.0674724280834198\nEpisode 193/1000, Total Reward: -470, Epsilon: 0.380 , loss: 0.02202017419040203\nEpisode 194/1000, Total Reward: -452, Epsilon: 0.378 , loss: 0.1464533656835556\nEpisode 195/1000, Total Reward: -298, Epsilon: 0.376 , loss: 0.05048171058297157\nEpisode 196/1000, Total Reward: -434, Epsilon: 0.374 , loss: 0.06507294625043869\nEpisode 197/1000, Total Reward: -479, Epsilon: 0.373 , loss: 0.01512162759900093\nEpisode 198/1000, Total Reward: -425, Epsilon: 0.371 , loss: 0.5704307556152344\nEpisode 199/1000, Total Reward: -112, Epsilon: 0.369 , loss: 0.09271001070737839\nEpisode 200/1000, Total Reward: -380, Epsilon: 0.367 , loss: 0.06301151216030121\nEpisode 201/1000, Total Reward: -255, Epsilon: 0.365 , loss: 0.09761371463537216\nEpisode 202/1000, Total Reward: -311, Epsilon: 0.363 , loss: 0.01181760523468256\nEpisode 203/1000, Total Reward: -161, Epsilon: 0.361 , loss: 0.012910737656056881\nEpisode 204/1000, Total Reward: -172, Epsilon: 0.360 , loss: 0.004920346196740866\nEpisode 205/1000, Total Reward: -325, Epsilon: 0.358 , loss: 0.07617770880460739\nEpisode 206/1000, Total Reward: -443, Epsilon: 0.356 , loss: 0.006099323276430368\nEpisode 207/1000, Total Reward: -24, Epsilon: 0.354 , loss: 0.004142691846936941\nEpisode 208/1000, Total Reward: -122, Epsilon: 0.353 , loss: 0.0036236129235476255\nEpisode 209/1000, Total Reward: -269, Epsilon: 0.351 , loss: 0.00265535362996161\nEpisode 210/1000, Total Reward: -148, Epsilon: 0.349 , loss: 0.1600896716117859\nEpisode 211/1000, Total Reward: -20, Epsilon: 0.347 , loss: 0.07905629277229309\nEpisode 212/1000, Total Reward: -89, Epsilon: 0.346 , loss: 0.06048155948519707\nEpisode 213/1000, Total Reward: -125, Epsilon: 0.344 , loss: 0.022038495168089867\nEpisode 214/1000, Total Reward: -398, Epsilon: 0.342 , loss: 0.009673189371824265\nEpisode 215/1000, Total Reward: -26, Epsilon: 0.340 , loss: 0.010501385666429996\nEpisode 216/1000, Total Reward: -461, Epsilon: 0.339 , loss: 0.1164514347910881\nEpisode 217/1000, Total Reward: -190, Epsilon: 0.337 , loss: 0.005264949519187212\nEpisode 218/1000, Total Reward: 12, Epsilon: 0.335 , loss: 0.29956719279289246\nEpisode 219/1000, Total Reward: -42, Epsilon: 0.334 , loss: 0.26662197709083557\nEpisode 220/1000, Total Reward: -201, Epsilon: 0.332 , loss: 0.14810357987880707\nEpisode 221/1000, Total Reward: -91, Epsilon: 0.330 , loss: 0.03991241753101349\nEpisode 222/1000, Total Reward: -116, Epsilon: 0.329 , loss: 0.04430345818400383\nEpisode 223/1000, Total Reward: -62, Epsilon: 0.327 , loss: 0.016695119440555573\nEpisode 224/1000, Total Reward: -292, Epsilon: 0.325 , loss: 0.010224142111837864\nEpisode 225/1000, Total Reward: -58, Epsilon: 0.324 , loss: 0.047973427921533585\nEpisode 226/1000, Total Reward: -371, Epsilon: 0.322 , loss: 0.08902069926261902\nEpisode 227/1000, Total Reward: -56, Epsilon: 0.321 , loss: 0.03346175700426102\nEpisode 228/1000, Total Reward: -84, Epsilon: 0.319 , loss: 0.03709150850772858\nEpisode 229/1000, Total Reward: -371, Epsilon: 0.317 , loss: 0.03309466317296028\nEpisode 230/1000, Total Reward: -47, Epsilon: 0.316 , loss: 0.1663047820329666\nEpisode 231/1000, Total Reward: -65, Epsilon: 0.314 , loss: 0.03813793882727623\nEpisode 232/1000, Total Reward: -3, Epsilon: 0.313 , loss: 0.010878062807023525\nEpisode 233/1000, Total Reward: -398, Epsilon: 0.311 , loss: 0.018970495089888573\nEpisode 234/1000, Total Reward: -4, Epsilon: 0.309 , loss: 0.050196122378110886\nEpisode 235/1000, Total Reward: -286, Epsilon: 0.308 , loss: 0.009612789377570152\nEpisode 236/1000, Total Reward: -362, Epsilon: 0.306 , loss: 0.04210129380226135\nEpisode 237/1000, Total Reward: -223, Epsilon: 0.305 , loss: 0.022565646097064018\nEpisode 238/1000, Total Reward: -37, Epsilon: 0.303 , loss: 0.06639166176319122\nEpisode 239/1000, Total Reward: -272, Epsilon: 0.302 , loss: 0.03566685691475868\nEpisode 240/1000, Total Reward: -100, Epsilon: 0.300 , loss: 0.10004476457834244\nEpisode 241/1000, Total Reward: -79, Epsilon: 0.299 , loss: 0.08574005216360092\nEpisode 242/1000, Total Reward: 6, Epsilon: 0.297 , loss: 0.04277599975466728\nEpisode 243/1000, Total Reward: -133, Epsilon: 0.296 , loss: 0.021324163302779198\nEpisode 244/1000, Total Reward: -46, Epsilon: 0.294 , loss: 0.0448833592236042\nEpisode 245/1000, Total Reward: -25, Epsilon: 0.293 , loss: 0.02236129529774189\nEpisode 246/1000, Total Reward: -283, Epsilon: 0.291 , loss: 0.041398562490940094\nEpisode 247/1000, Total Reward: -195, Epsilon: 0.290 , loss: 0.00828083511441946\nEpisode 248/1000, Total Reward: -389, Epsilon: 0.288 , loss: 0.006177602335810661\nEpisode 249/1000, Total Reward: -371, Epsilon: 0.287 , loss: 0.19500987231731415\nEpisode 250/1000, Total Reward: -68, Epsilon: 0.286 , loss: 0.07349761575460434\nEpisode 251/1000, Total Reward: -152, Epsilon: 0.284 , loss: 0.041563909500837326\nEpisode 252/1000, Total Reward: -398, Epsilon: 0.283 , loss: 0.010531562380492687\nEpisode 253/1000, Total Reward: -335, Epsilon: 0.281 , loss: 0.0035739776212722063\nEpisode 254/1000, Total Reward: -91, Epsilon: 0.280 , loss: 0.005187762435525656\nEpisode 255/1000, Total Reward: -299, Epsilon: 0.279 , loss: 0.017712431028485298\nEpisode 256/1000, Total Reward: 10, Epsilon: 0.277 , loss: 0.005238072481006384\nEpisode 257/1000, Total Reward: -104, Epsilon: 0.276 , loss: 0.015363816171884537\nEpisode 258/1000, Total Reward: -63, Epsilon: 0.274 , loss: 0.06536390632390976\nEpisode 259/1000, Total Reward: -85, Epsilon: 0.273 , loss: 0.0013630312168970704\nEpisode 260/1000, Total Reward: -295, Epsilon: 0.272 , loss: 0.03959956020116806\nEpisode 261/1000, Total Reward: -17, Epsilon: 0.270 , loss: 0.2544935643672943\nEpisode 262/1000, Total Reward: -196, Epsilon: 0.269 , loss: 0.007444608956575394\nEpisode 263/1000, Total Reward: -7, Epsilon: 0.268 , loss: 0.24740523099899292\nEpisode 264/1000, Total Reward: -128, Epsilon: 0.266 , loss: 0.00959989707916975\nEpisode 265/1000, Total Reward: 1, Epsilon: 0.265 , loss: 0.01070189569145441\nEpisode 266/1000, Total Reward: -407, Epsilon: 0.264 , loss: 0.02919013798236847\nEpisode 267/1000, Total Reward: -425, Epsilon: 0.262 , loss: 0.053984079509973526\nEpisode 268/1000, Total Reward: -148, Epsilon: 0.261 , loss: 0.27373170852661133\nEpisode 269/1000, Total Reward: -50, Epsilon: 0.260 , loss: 0.18726186454296112\nEpisode 270/1000, Total Reward: -353, Epsilon: 0.258 , loss: 0.05565563216805458\nEpisode 271/1000, Total Reward: -344, Epsilon: 0.257 , loss: 0.010792780667543411\nEpisode 272/1000, Total Reward: -212, Epsilon: 0.256 , loss: 0.013682946562767029\nEpisode 273/1000, Total Reward: -78, Epsilon: 0.255 , loss: 0.031204024329781532\nEpisode 274/1000, Total Reward: -322, Epsilon: 0.253 , loss: 0.017329370602965355\nEpisode 275/1000, Total Reward: -341, Epsilon: 0.252 , loss: 0.002544211456552148\nEpisode 276/1000, Total Reward: -389, Epsilon: 0.251 , loss: 0.011533492244780064\nEpisode 277/1000, Total Reward: -342, Epsilon: 0.249 , loss: 0.23077251017093658\nEpisode 278/1000, Total Reward: -380, Epsilon: 0.248 , loss: 0.07380370795726776\nEpisode 279/1000, Total Reward: -47, Epsilon: 0.247 , loss: 0.013143195770680904\nEpisode 280/1000, Total Reward: -130, Epsilon: 0.246 , loss: 0.10956663638353348\nEpisode 281/1000, Total Reward: -371, Epsilon: 0.245 , loss: 0.017848839983344078\nEpisode 282/1000, Total Reward: -23, Epsilon: 0.243 , loss: 0.015311491675674915\n","output_type":"stream"}],"execution_count":null}]}